<p>Project 1:</p>
<p>For our Unemployment (from lmi2.detma.org and retrieved from datamechanics.io) and 2018 Governor Election dataset (from wbur.org), we did a projection on the Unemployment dataset and only kept the City and the unployment rate£¨as of Dec 2018£©fields. 
We did the same thing on the Governor dataset and kept the Town and the voting percentage for Charlie Baker (2018 MA governor election) fields.
The City and the Town fields are the keys in these two datasets and they have the exact same contents, which is the name of all the towns in Massachusetts. Finally, we join the two datasets together by using the Town field as key and show the unemployment rate and supporting rate for Charlie Baker. 
The resulting dataset can help us to observe the relationship between the unemployment rate and supporting rate for a Republican governor. Therefore, political analysts will be able to use this dataset to understand which party did unemployed people really support.

For the Spark Governor dataset, we collected data from PD43 and created this dataset in order to satifised the requirnment of our Spark partner. This dataset contains all the governor candidates from the elections during 2002-2018.We uploaded this file to datamechanics.io and retrieved it. For this dataset, we did a projection to only keep the Party field and did a aggregate to count the total number of candidates of each party. The result shows that the Republican and Democrat Party both have an overwhelming number of candidates while the Libertarian and Green-Rainbow Party had a fewer number of candidates. This result can be viewed as a microcosm of the political power distribution in Massachusetts.

For the Health and Uber dataset, first of all,at the health_uber_input.py, we retrieve the health dataset from http://datamechanics.io/ and uber dataset from github, these can be counted as from 2 different data portals. After retrieving the data online, we store them to mongodb collection chuci_yfch_yuwan_zhurh.uber and chuci_yfch_yuwan_zhurh.health separatly. Then in the health_uber_output.py, we firstly aggregate chuci_yfch_yuwan_zhurh.health, use CityName as the key and find each CityName's max population, secondly we aggregate chuci_yfch_yuwan_zhurh.uber, use CITY as the key and find each CITY's mean travel time.Finally, we join two dataset together to see each city's population and mean travel time. Since mongodb don't support traditional join, so I use python to do the join work. We want to figure out if there is a relation between a city's population and mean travel time. Unfortunately, due to the limit of data and city, so far we don't find any obvious relation between them.</p>

<p>Project 2:</p>
<p>For the request of implementing a constraint satisfaction or optimization technique, we decide to implement k-mean over our dataset. Since we have the data of uber travel's destination name, we can use these destination name to get the destination's (latitude, longitude) by requesting Google Map API. But we cannot put this part in our script since each time's request will cost money and we cannot afford to request it again and again. So we directly load the data with (latitude, longitude) into the MongoDB. With the data with (latitude, longitude), we can implement k-mean to see how we can divide the destination into several groups of areas with various k value. And below is what we get. We also visualize the data by drawing a scatter plot to display each group of center point, in order to get an intuitive insight of the data.</p>
<p><img src="https://github.com/yizheshexin/course-2019-spr-proj/blob/master/chuci_yfch_yuwan_zhurh/fig_10.png" style="max-width:100%;"></p>
<p><img src="https://github.com/yizheshexin/course-2019-spr-proj/blob/master/chuci_yfch_yuwan_zhurh/kmean.png" style="max-width:100%;"></p>
<p>For the request of implementing a statistical analysis, we choose to compute correlation coefficient and p-value over several groups of data to see if there are correlation between them or not. We use both the functions that are defined in slide and the scipy library to calculate the data. First of all, we analyze the relation between uber's distance and uber's travel time. In fact, even we don't calculate the correlation coefficient, with our intuition, the two variables should be positive related. The correlation coefficient is in line with our expectation, 0.8402 means that two variables are highly positive related. And the p-value is almost 0. When we do hypothesis test, our H0 should be: uber's distance and uber's travel time are not related, our H1 should be: uber's distance and uber's travel time are related, we could set the significance level alpha = 0.05, since 0 < 0.05, we should reject H0, accept H1, so uber's distance and uber's travel time are related. </p>
<p>Now let's see some more interesting data sets. Let's try to find out if there is realtion between GOP's vote percent and unemplotment rate. This time I don't think we can judge it by our intuition so we will rely on the correlation coefficient. We choose 3 days to calculate the correlation coefficient and the value is between -0.145 ~ -0.081. It seems that these two variables are very weakly negative related.</p>  
<p><img src="https://github.com/yizheshexin/course-2019-spr-proj/blob/master/chuci_yfch_yuwan_zhurh/stat.png" style="max-width:100%;"></p>

<p>
Project3:
</p>
<p>
For the first visualization graph, visit the `project3_webservice.html`. For the second one, please first put `index.html` and `final_json.json` in the same folder, then run `http-server .` on your terminal. After that, you should get a url from your terminal. By visiting this url, you should be able to view a graph like this:
<img src="https://github.com/yizheshexin/course-2019-spr-proj/blob/master/chuci_yfch_yuwan_zhurh/project3_visualization2.png" style="max-width:100%;">
</p>
